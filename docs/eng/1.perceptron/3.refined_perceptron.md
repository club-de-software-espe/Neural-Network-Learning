# improving upon the simple perceptron

As we progressed forward in this examples, we came to realize that in particular, this was a trivial one, and not very useful. What we want to do now is to solve some of its problems, and mainly expand the prediction capabilities, such as it not only guesses _f(x) = x_ but also _f(x) = mx + b_, so you can check [RefinedPerceptron.py](/nn_lib/Perceptron/RefinedPerceptron.py) for the improvements and [refined_perceptron_sample.py](/nn_lib/samples/refined_perceptron_sample.py) for the example

## Current issues

- The perceptron cannot deal with all of its inputs being _&varnothing;_, if we go back to our first calculation definition we have: _x<sub>0</sub> * w<sub>0</sub> + x<sub>1</sub> * w<sub>1</sub> + ... +  x<sub>n</sub> * w<sub>n</sub>_ . Here we already can spot the problem, if in _x<sub>0</sub> to x<sub>n</sub>_ all _x_ values are _&varnothing;_  then the output will not be adjusted in spite of the weight change.

  - Solution: introduce a new input, called a _bias_ that in this case is always going to be _1_ then even if all inputs are _&varnothing;_ we can still adjust the bias weight to get a meaningful result. Now  we have: _x<sub>0</sub> * w<sub>0</sub> + x<sub>1</sub> * w<sub>1</sub> + ... +  x<sub>n</sub> * w<sub>n</sub> + **b<sub>1</sub>*w<sub>b<sub>1</sub></sub>**_ where _b<sub>1</sub> = 1_

- The perceptron can only classify points above or below _f(x) = x_, we want to generalize it to _f(x) = mx + b_

  - Solution: we change the training data set to use the general equation for a line _y = mx +b_, we will call this training set `refined_set`.
  To provide the labels for this training set we will consider this simple comparison: instead of having `1 if x > y` we will have `1 if y > line_y` with `line_y` being the _y_ value for x such as _(x, y)_ satisfy _y = mx +b_

## For the visual representation

We create a function `predict_line_y` in the `RefinedPerceptron.Perceptron` class, to predict _y_ values for any given _x_ value as an input, with the weights that the perceptron has tuned so far.
To make such function we need to use the weights to resemble the line equation.

For a linear perceptron (two inputs, one bias), we have:

_x<sub>0</sub> * w<sub>0</sub> + x<sub>1</sub> * w<sub>1</sub> + b<sub>1</sub>*w<sub>b<sub>1</sub></sub> = 0_ with _b<sub>1</sub> = 1_

To visualize it better we can change _x<sub>0</sub>_ and _x<sub>1</sub>_ for _x_ and _y_ so now we have:

_x * w<sub>0</sub> + y * w<sub>1</sub> + w<sub>b<sub>1</sub></sub> = 0_

If we solve for _y_ then:

_y = - ( <sup>w<sub>b<sub>1</sub></sub></sup> &frasl;<sub>w<sub>1</sub></sub> ) - ( <sup>w<sub>0</sub></sup> &frasl;<sub>w<sub>1</sub></sub> )* x_

We embed this equation in our code, and now we can visualize which line the perceptron is approximating.

[next](/docs/eng/2.multilayer_perceptron/1.perceptron_problems.md)