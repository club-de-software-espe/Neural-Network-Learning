# Some things that we need to build our own NN

Now a days using a neural network is a relatively easy task, many libraries and documentation will guide you towards the right path (such as Tensorflow). In this case, we will focus not only in using neural networks, but how they internally works, how the math happens and why is important to know the basics, before we start to tinker around with more robust pieces of software for machine-learning.

## Basics of the architecture

A neural network typically has three things:

- Input values
- A hidden layer
- An output layer

Each layer with an independent number of neurons (perceptronâ€™s) _note: we are building a fully connected network_

## Feed forward

This process allows the neural network to perform calculations between its layers. The way it works goes as follows:

1. All weights in the network are initialized. _(This one is a broad topic, there are many different strategies for this, but we will go as simple as we can)_
1. The inputs are feed to the input layer
1. Each hidden neuron computes the _weighted sum_ **(we have discussed this in our [perceptron](/docs/eng/1.perceptron/1.introduction.md) example)**
1. After that output is feed to an activation function and then passed to the new layer
1. We repeat this process for all layers

However, with an arbitrary number of neurons for each layer, then, how do we keep track of all the weights and operations that we need to do?

### Linear algebra

The classic or standard approach is to use **linear algebra**, mainly **matrix math** to store and perform the calculations that we need.

Let us look at a simple neural network, two inputs, two hidden neurons, and one output neuron:

![nn_matrix_math](/docs/img/nn_matrix_repr.jpg)

We name the inputs as x<sub>0</sub> and x<sub>1</sub>

For the weights between the input and hidden layers, we name them using this scheme: w<sub>origin_neuron, next_neuron</sub>

Now we can arrange the weights and inputs as matrices such as:

For the inputs, we end up with a 2 x 1 matrix:

![input_matrix_math](/docs/img/input_matrix_ex.jpg)

And for the weights, a 2 x 2 matrix:

![weight_matrix_math](/docs/img/weight_matrix_ex.jpg)

Quick note: _the sub indexes of the weight matrix do not represent rows x columns, they represent the order that we need to produce the outputs_

At each of the hidden neurons, we need to perform that _weighted sum_, so how exactly does matrix representation helps us.

If we describe the operations we need we have:

_h<sub>0</sub> = x<sub>0</sub> * w<sub>11</sub> + x<sub>1</sub> * w<sub>21</sub>_

And

_h<sub>1</sub> = x<sub>0</sub> * w<sub>12</sub> + x<sub>1</sub> * w<sub>22</sub>_

(_h<sub>0_ and _h<sub>1_ are the outputs at the hidden layer neurons 1 and 2)

It so happens that **[matrix multiplication](https://en.wikipedia.org/wiki/Matrix_multiplication)** is defined in that way, now we can represent all our weighted sums like this:

(This process is the same between all layers)

![matrix_mult_math](/docs/img/matmul_ex.jpg)

To skip the process of making a matrix library, we will use [numpy](http://www.numpy.org/)

In next documents, I will explain how to use this matrix math within the neural network library and exactly the steps I followed.

### Bias

As we have discussed on the previous perceptron example, what happens if all inputs are &varnothing;?, this is a problem we need to solve. Last time we introduced a variable called **Bias**, it allowed us to take the output in a certain direction.

How do we implement this variable in the multi-layer concept?

As simple as this, we are going to say that:

_H_ is the output matrix for the hidden layer, _W_ is the weight matrix, and _I_ is the input matrix.

We will introduce a new matrix _B_ the same dimensions as the hidden layer, and now simply add it to the result of the matrix multiplication between _W_ and _I_, so now we have: _H = I x W + B_

(We introduce a **bias** for every layer)

### Activation function

Now that we have our _H_ matrix, we need to conform those values to a certain range, the most common way of doing this is called a _sigmoid function_ that we need to apply to all elements in the output matrix, so now we have: _H = &sigma;( I x W + B )_ _) ( Sigma becomes an elementwise operation)_. Note that _&sigma;(x) = <sup>1</sup>&frasl;<sub>( 1 + &ee; <sup>**-x**</sup> )</sub>_ Now every value on the _H_ matrix will get converted into a range _[0, 1]_.

You can look more in depth to the sigmoid function [here](https://en.wikipedia.org/wiki/Sigmoid_function)

[next](/docs/eng/2.multilayer_perceptron/3.training_a_neural_network.md)