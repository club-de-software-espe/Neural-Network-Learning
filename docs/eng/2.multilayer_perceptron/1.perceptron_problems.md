# Limitations and problems with a single perceptron

When we talk about machine learning, and problem solving in general, we often refer to more complex problems than the ones that we have seen here. Perhaps you have wondered about how a perceptron could take more than 2 inputs, and provide more than one output, but it simply can't, in 1969 Marvin Minsky and Seymour Papert demonstrated in their book "Perceptrons", how is that they are only suitable to solve "linearly separable" problems.

However, what does "linearly separable" mean? Simply put, it means that you can classify a set of points dividing them with a line, such as in our [previous examples](/nn_lib/samples/perceptron_samples). Often times, we do not have "linearly separable" data, in fact most interesting problems are not.

But first we will visit a simple one:

## The XOR problem

To better understand the limitations of a perceptron, we could start with some basic Boolean operators, as they are simple enough. In the world of mathematical logic, we have operators such as **AND** _(often represented as && in programming)_ and **OR** _(often represented as &parallel; in programming)_, both of this are linearly separable, let us look at their truth tables:

- AND

    For A&&B (A&wedge;B):

    | A | B | Out |
    |---|---|:---:|
    | T | T |  T  |
    | T | F |  F  |
    | F | T |  F  |
    | F | F |  F  |

    We can draw a straight line that separates True and False.

    ![and_truth_table](/docs/img/and_truth_table.jpg)

- OR

    Same for A&parallel;B (A&vee;B)

    | A | B | Out |
    |---|---|:---:|
    | T | T |  T  |
    | T | F |  T  |
    | F | T |  T  |
    | F | F |  F  |

    As before, we can separate T and F with a line.

    ![or_truth_table](/docs/img/or_truth_table.jpg)

XOR or Exclusive OR is another logical/Boolean operator that has its output set to true, only if both inputs have different values.

- **XOR**

    Now we have an issue, we will see that we cannot separate the truth table with one straight line

    A&veebar;B (A&oplus;B)

    | A | B | Out |
    |---|---|:---:|
    | T | T |  F  |
    | T | F |  T  |
    | F | T |  T  |
    | F | F |  F  |

    Here we have a better way to visualize the case

    ![xor_truth_table](/docs/img/xor_truth_table.jpg)

### How to solve the XOR problem

Here is where the multi-layer perceptron concept comes in handy, if we decompose _A&veebar; B_ as _(A&vee; B)&wedge;&not;(A&wedge; B)_ then we see that we can solve this non "linearly-separable" problem by spliting it into three linearly separable ones, as: _AND (&wedge; )_, _NOT-AND (&not;&wedge; )_ and _OR (&vee; )_. In a multi-layer perceptron we translate this as having one perceptron that learns each operation, and then combining the outputs such as we have _XOR (&veebar; )_.

![xor_as_thre_linearly_separable_problems](/docs/img/xor_representation.jpg)

[next](/docs/eng/2.multilayer_perceptron/2.designing_a_neural_network.md)