# Algunas consideraciones para construir nuestra propia red neuronal

En la actualidad, usar una red neuronal es un proceso relativamente sencillo, existen muchas librerías que pueden guiarnos hacia el camino correcto, como por ejemplo Tensorflow. En este caso, nos enfocaremos no solo en cómo usar la red neuronal, sino que también en cómo funciona internamente, como se realizan los cálculos matemáticos y, sobre todo, porque es importante conocer sobre las bases, antes de empezar a jugar con piezas más fuertes de software para aprendizaje de máquina.

## Partes básicas de la arquitectura

Una red neuronal típicamente está formada por tres elementos:

- Valores de entrada
- Una capa oculta
- Una capa de salida

Cada una de las capas tiene un número independiente de neuronas (perceptrones) _nota: estamos construyendo una red completamente conectada_

## Feed Forward

Este proceso es el que permite que la red neuronal haga los cálculos necesarios entre sus capas, la forma en que funciona es la siguiente:

1. Todos los pesos de la red son inicializados. _(Este tema en si es bastante extenso, existen muchas estrategias diferentes de inicialización, pero lo mantendremos lo más simple posible)_
1. Las entradas son introducidas en la capa de entrada
1. Cada neurona de la capa escondida calcula la suma de los pesos multiplicados por las entradas _weighted sum_ **(la cual hemos discutido en nuestro ejemplo [perceptrón](/docs/spa/1.perceptron/1.introducion.md))**
1. La salida es enviada hacia una función de activación antes de ser enviada a la siguiente capa.
1. Repetimos el proceso para todas las capas.

¿Aun así, con un número arbitrario de neuronas para cada una de las capas, como mantenemos la cuenta de todos los pesos y operaciones que debemos hacer?

### Algebra lineal

La manera "clásica" o estándar de realizar estas operaciones es mediante **algebra lineal**, principalmente **operaciones entre matrices** para guardar y realizar los cálculos que necesitamos.

Tomemos un ejemplo bastante simple, para ilustrar mejor el uso del algebra lineal en estos casos. Tendremos: dos entradas, dos neuronas escondidas y una neurona de salida:

![nn_matrix_math](/docs/img/nn_matrix_repr.jpg)

Representaremos las entradas como x<sub>0</sub> y x<sub>1</sub>

Para cada uno de los pesos entre la capa de entrada y la capa oculta, dispondremos sus nombres usando el siguiente esquema: w<sub>origin, next_neuron</sub>

Ahora podemos arreglar los pesos y entradas como matrices, de la siguiente manera:

Para las entradas tenemos una matriz de 2 x 1:

![input_matrix_math](/docs/img/input_matrix_ex.jpg)

Para los pesos tenemos una matriz de 2 x 2:

![weight_matrix_math](/docs/img/weight_matrix_ex.jpg)

Nota: _Los subíndices de la matriz de pesos no representan filas x columnas, se han dispuesto de esa manera porque representan el orden por el cual necesitamos realizar las operaciones para producir las salidas adecuadas_

¿En cada una de las neuronas de la capa escondida necesitamos realizar esta operación de suma y multiplicación de pesos por entradas _weighted sum_, así que como exactamente la representación en matrices nos ayuda?

Si describimos las operaciones que necesitamos, tenemos:

_h<sub>0</sub> = x<sub>0</sub> * w<sub>11</sub> + x<sub>1</sub> * w<sub>21</sub>_

Y

_h<sub>1</sub> = x<sub>0</sub> * w<sub>12</sub> + x<sub>1</sub> * w<sub>22</sub>_

(_h<sub>0_ y _h<sub>1_ son las salidas de las neuronas escondidas 1 y 2)

De hecho, sucede que la **[multiplicación de matrices](https://es.wikipedia.org/wiki/Multiplicaci%C3%B3n_de_matrices)** esta definida en esta manera, ahora podemos representar nuestras operaciones en esta forma:

(Este proceso es el mismo entre todas las capas)

![matrix_mult_math](/docs/img/matmul_ex.jpg)

Para saltarnos el proceso de realizar estas operaciones nosotros, usaremos una librería como [numpy](http://www.numpy.org/)

En los siguientes documentos, explicare como es que se usan estas operaciones de matrices dentro de la red neuronal y como es que exactamente se implementa, además de los pasos para ello.

### Bias

Como hemos visto antes en nuestros ejemplos del perceptrón, ¿Qué es lo que sucede si todas las entradas son &varnothing;? Este es un problema que debemos resolver. Previamente lo hicimos mediante la introducción de una nueva variable llamada **Bias**, la cual nos permitía manejar esta situación, empujando la salida hacia una dirección especifica.

¿Cómo implementamos esto en el concepto de multi-capa?

Simple, vamos a establecer lo siguiente:

_H_ es la matriz salida para la capa oculta, _W_ es la matriz de pesos e _I_ es la matriz de entradas.

Ahora introduciremos una nueva matriz _B_ con las mismas dimensiones que la capa oculta, simplemente la sumaremos al resultado del producto de la matriz de entrada y la matriz de pesos _I x W_ así que ahora tenemos: _H = I x W + B_

(Introduciremos un **bias** para cada capa)

### Función de activación

Teniendo nuestra matriz _H_, tenemos que ajustar esos valores a un rango especifico, la manera más común para ello era aplicar a cada elemento de la matriz una función _sigmoide_, así que ahora nuestra matriz _H_ será: H = &sigma;( I x W + B )_ _) ( Sigma se vuelve una función que se aplica a cada elemento de la matriz)_. Notemos que _&sigma;(x) = <sup>1</sup>&frasl;<sub>( 1 + &ee; <sup>**-x**</sup> )</sub>_ Ahora cada valor en la matriz _H_ será convertido al rango _[0, 1]_.

Puedes revisar mucho más a fondo sobre la función sigmoide [aquí](https://es.wikipedia.org/wiki/Funci%C3%B3n_sigmoide)

[siguiente](docs/spa/2.percpetron_multicapa/3.entrenando _una_red_neuronal.md)